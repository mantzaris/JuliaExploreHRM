{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7bb1d18a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m  Activating\u001b[22m\u001b[39m project at `~/Documents/repos/JuliaExploreHRM`\n"
     ]
    }
   ],
   "source": [
    "using Pkg\n",
    "Pkg.activate(\"..\")  #one level up, where Project.toml lives\n",
    "Pkg.instantiate()   #download/install anything missing\n",
    "# Pkg.status();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9e4a070a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: replacing module HRMCommon.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TaskLocalRNG()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "include(joinpath(@__DIR__, \"..\", \"hrm_common.jl\"))\n",
    "using .HRMCommon\n",
    "using Statistics, Random, Test\n",
    "using LinearAlgebra: norm\n",
    "rng = Random.default_rng()\n",
    "\n",
    "# Y1, st1 = HRMCommon.transformer_forward!(blk, ps, st, X1)\n",
    "# Y2, st2 = HRMCommon.transformer_forward!(blk, ps, st1, X2)  # reuse states\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "124f6fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "d, L, B = 8, 4, 2\n",
    "X = randn(Float32, d, L, B)\n",
    "\n",
    "blk = HRMCommon.make_transformer_block(d; nheads=2, ff_mult=2, pos_kind=:sinusoidal)\n",
    "\n",
    "ps = (\n",
    "  pos_layer = Lux.setup(rng, blk.pos_layer)[1],\n",
    "  mha  = Lux.setup(rng, blk.mha )[1],\n",
    "  ln1  = Lux.setup(rng, blk.ln1 )[1],\n",
    "  ff   = Lux.setup(rng, blk.ff  )[1],\n",
    "  ln2  = Lux.setup(rng, blk.ln2 )[1],\n",
    ")\n",
    "st = (\n",
    "  pos_layer = Lux.setup(rng, blk.pos_layer)[2],\n",
    "  mha  = Lux.setup(rng, blk.mha )[2],\n",
    "  ln1  = Lux.setup(rng, blk.ln1 )[2],\n",
    "  ff   = Lux.setup(rng, blk.ff  )[2],\n",
    "  ln2  = Lux.setup(rng, blk.ln2 )[2],\n",
    ")\n",
    "\n",
    "Y, st2 = HRMCommon.transformer_forward!(blk, ps, st, X)\n",
    "@assert size(Y) == (d, L, B)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "28903d42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a240a8f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[1mTest Summary:         | \u001b[22m\u001b[32m\u001b[1mPass  \u001b[22m\u001b[39m\u001b[36m\u001b[1mTotal  \u001b[22m\u001b[39m\u001b[0m\u001b[1mTime\u001b[22m\n",
      "HRMCommon - utilities | \u001b[32m   9  \u001b[39m\u001b[36m    9  \u001b[39m\u001b[0m0.0s\n",
      "\u001b[0m\u001b[1mTest Summary:                                 | \u001b[22m\u001b[32m\u001b[1mPass  \u001b[22m\u001b[39m\u001b[36m\u001b[1mTotal  \u001b[22m\u001b[39m\u001b[0m\u001b[1mTime\u001b[22m\n",
      "HRMCommon - positional encodings (sinusoidal) | \u001b[32m   4  \u001b[39m\u001b[36m    4  \u001b[39m\u001b[0m0.0s\n",
      "\u001b[0m\u001b[1mTest Summary:                                              | \u001b[22m\u001b[32m\u001b[1mPass  \u001b[22m\u001b[39m\u001b[36m\u001b[1mTotal  \u001b[22m\u001b[39m\u001b[0m\u001b[1mTime\u001b[22m\n",
      "HRMCommon - transformer block (shapes + residual identity) | \u001b[32m   3  \u001b[39m\u001b[36m    3  \u001b[39m\u001b[0m0.0s\n",
      "\u001b[0m\u001b[1mTest Summary:         | \u001b[22m\u001b[32m\u001b[1mPass  \u001b[22m\u001b[39m\u001b[36m\u001b[1mTotal  \u001b[22m\u001b[39m\u001b[0m\u001b[1mTime\u001b[22m\n",
      "HRMCommon - gradients | \u001b[32m   8  \u001b[39m\u001b[36m    8  \u001b[39m\u001b[0m0.9s\n",
      "\u001b[0m\u001b[1mTest Summary:                        | \u001b[22m\u001b[32m\u001b[1mPass  \u001b[22m\u001b[39m\u001b[36m\u001b[1mTotal  \u001b[22m\u001b[39m\u001b[0m\u001b[1mTime\u001b[22m\n",
      "HRMCommon - variable sequence length | \u001b[32m   2  \u001b[39m\u001b[36m    2  \u001b[39m\u001b[0m0.0s\n",
      "\u001b[0m\u001b[1mTest Summary:                  | \u001b[22m\u001b[32m\u001b[1mPass  \u001b[22m\u001b[39m\u001b[36m\u001b[1mTotal  \u001b[22m\u001b[39m\u001b[0m\u001b[1mTime\u001b[22m\n",
      "MHA attention weights sum to 1 | \u001b[32m   1  \u001b[39m\u001b[36m    1  \u001b[39m\u001b[0m0.4s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Test.DefaultTestSet(\"MHA attention weights sum to 1\", Any[], 1, false, false, true, 1.756352106715557e9, 1.756352107069145e9, false, \"/home/resort/Documents/repos/JuliaExploreHRM/04_transformer_blocks_common/jl_notebook_cell_df34fa98e69747e1a8f8a730347b8e2f_W4sZmlsZQ==.jl\")"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Small helper to build ps/st safely even when pos_layer is `nothing`\n",
    "function setup_parameters_and_state(rng, blk)\n",
    "    if blk.pos_layer === nothing\n",
    "        pos_ps, pos_st = (;), (;)\n",
    "    else\n",
    "        pos_ps, pos_st = Lux.setup(rng, blk.pos_layer)\n",
    "    end\n",
    "    mha_ps, mha_st = Lux.setup(rng, blk.mha)\n",
    "    ln1_ps, ln1_st = Lux.setup(rng, blk.ln1)\n",
    "    ff_ps,  ff_st  = Lux.setup(rng, blk.ff)\n",
    "    ln2_ps, ln2_st = Lux.setup(rng, blk.ln2)\n",
    "\n",
    "    ps = (; pos_layer=pos_ps, mha=mha_ps, ln1=ln1_ps, ff=ff_ps, ln2=ln2_ps)\n",
    "    st = (; pos_layer=pos_st, mha=mha_st, ln1=ln1_st, ff=ff_st, ln2=ln2_st)\n",
    "    return ps, st\n",
    "end\n",
    "\n",
    "# Mutate-in-place helper to zero all MHA + FF weights (keeps NamedTuples intact)\n",
    "function zero_mha_and_ff!(ps)\n",
    "    # Multi-head attention projections\n",
    "    ps.mha.q_proj.weight .= 0\n",
    "    ps.mha.k_proj.weight .= 0\n",
    "    ps.mha.v_proj.weight .= 0\n",
    "    ps.mha.out_proj.weight .= 0\n",
    "    # FFN layers\n",
    "    ps.ff.layer_1.weight .= 0\n",
    "    ps.ff.layer_1.bias   .= 0\n",
    "    ps.ff.layer_2.weight .= 0\n",
    "    ps.ff.layer_2.bias   .= 0\n",
    "    return ps\n",
    "end\n",
    "\n",
    "\n",
    "\n",
    "@testset \"HRMCommon - utilities\" begin\n",
    "    d, L, B = 8, 5, 3\n",
    "\n",
    "    # _as3d\n",
    "    X2 = randn(Float32, d, B)\n",
    "    X3 = randn(Float32, d, L, B)\n",
    "    Y3, was2d = HRMCommon._as3d(X2)\n",
    "    @test was2d == true\n",
    "    @test size(Y3) == (d, 1, B)\n",
    "    Y3b, was2d_b = HRMCommon._as3d(X3)\n",
    "    @test was2d_b == false\n",
    "    @test size(Y3b) == size(X3)\n",
    "\n",
    "    # apply_tokenwise vs explicit batched apply\n",
    "    ln = Lux.LayerNorm(d)\n",
    "    ps_ln, st_ln = Lux.setup(rng, ln)\n",
    "\n",
    "    # 2-D case\n",
    "    Y2_tok, _ = HRMCommon.apply_tokenwise(ln, ps_ln, st_ln, X2)\n",
    "    Y2_dir, _  = Lux.apply(ln, X2, ps_ln, st_ln)\n",
    "    @test size(Y2_tok) == size(X2)\n",
    "    @test eltype(Y2_tok) == eltype(X2)\n",
    "    @test Y2_tok ≈ Y2_dir\n",
    "\n",
    "    # 3-D case\n",
    "    Y3_tok, _ = HRMCommon.apply_tokenwise(ln, ps_ln, st_ln, X3)\n",
    "    Y2_dir2, _ = Lux.apply(ln, reshape(X3, d, L*B), ps_ln, st_ln)\n",
    "    @test size(Y3_tok) == size(X3)\n",
    "    @test Y3_tok ≈ reshape(Y2_dir2, d, L, B)\n",
    "end\n",
    "\n",
    "@testset \"HRMCommon - positional encodings (sinusoidal)\" begin\n",
    "    d, L, B = 8, 7, 2\n",
    "    X1 = randn(Float32, d, L, B)\n",
    "    X2 = randn(Float32, d, L, B)\n",
    "\n",
    "    pe, pe_apply = HRMCommon.make_positional_layer(d; kind=:sinusoidal)\n",
    "    ps_pe, st_pe = Lux.setup(rng, pe)\n",
    "\n",
    "    Y1, st1 = pe_apply(X1, pe, ps_pe, st_pe)\n",
    "    Y2, st2 = pe_apply(X2, pe, ps_pe, st1)\n",
    "\n",
    "    @test size(Y1) == (d, L, B)\n",
    "    @test eltype(Y1) == Float32\n",
    "\n",
    "    # The added positional term should depend only on positions (L,B), not on content of X.\n",
    "    P1 = Y1 .- X1\n",
    "    P2 = Y2 .- X2\n",
    "    @test P1 ≈ P2\n",
    "    @test any(!iszero, P1)  # it really adds something\n",
    "end\n",
    "\n",
    "@testset \"HRMCommon - transformer block (shapes + residual identity)\" begin\n",
    "    d, L, B = 8, 4, 2\n",
    "    X = randn(Float32, d, L, B)\n",
    "\n",
    "    # 1) No positional encodings; with zeroed projections FFN, block should be identity.\n",
    "    blk_none = HRMCommon.make_transformer_block(d; nheads=2, ff_mult=2,\n",
    "        attention_dropout_probability=0.0, pos_kind=:none)\n",
    "    ps_none, st_none = setup_parameters_and_state(rng, blk_none)\n",
    "\n",
    "    ps_id = deepcopy(ps_none)\n",
    "    zero_mha_and_ff!(ps_id)\n",
    "\n",
    "    Y_id, st_id = HRMCommon.transformer_forward!(blk_none, ps_id, st_none, X)\n",
    "    @test size(Y_id) == size(X)\n",
    "    @test Y_id ≈ X\n",
    "\n",
    "    # 2) Sinusoidal PEs; with zeroed projections FFN, output should be X + P (constant shift).\n",
    "    blk_pe = HRMCommon.make_transformer_block(d; nheads=2, ff_mult=2,\n",
    "        attention_dropout_probability=0.0, pos_kind=:sinusoidal)\n",
    "    ps_pe, st_pe = setup_parameters_and_state(rng, blk_pe)\n",
    "\n",
    "    ps_pe_id = deepcopy(ps_pe)\n",
    "    zero_mha_and_ff!(ps_pe_id)\n",
    "\n",
    "    Y_pe1, stp1 = HRMCommon.transformer_forward!(blk_pe, ps_pe_id, st_pe, X)\n",
    "    P_inferred1 = Y_pe1 .- X\n",
    "\n",
    "    # change input, same (L,B); inferred P should be identical\n",
    "    X_alt = randn(Float32, d, L, B)\n",
    "    Y_pe2, stp2 = HRMCommon.transformer_forward!(blk_pe, ps_pe_id, stp1, X_alt)\n",
    "    P_inferred2 = Y_pe2 .- X_alt\n",
    "    @test P_inferred1 ≈ P_inferred2\n",
    "end\n",
    "\n",
    "@testset \"HRMCommon - gradients\" begin\n",
    "    d, L, B = 8, 6, 3\n",
    "    X = randn(Float32, d, L, B)\n",
    "    T = randn(Float32, d, L, B)   # random target for MSE\n",
    "\n",
    "    blk = HRMCommon.make_transformer_block(d; nheads=2, ff_mult=2,\n",
    "        attention_dropout_probability=0.0, pos_kind=:sinusoidal)\n",
    "    ps, st = setup_parameters_and_state(rng, blk)\n",
    "\n",
    "    # Loss as a function of parameters (state treated as non-differentiable accumulator)\n",
    "    function loss(ps_blk)\n",
    "        Y, _ = HRMCommon.transformer_forward!(blk, ps_blk, st, X)\n",
    "        return sum(abs2, Y .- T) / length(Y)\n",
    "    end\n",
    "\n",
    "    g_ps = first(Zygote.gradient(loss, ps))\n",
    "\n",
    "    # Representative checks: gradients for attention and FFN exist and are nonzero\n",
    "    @test haskey(g_ps, :mha) && haskey(g_ps.mha, :q_proj)\n",
    "    @test g_ps.mha.q_proj.weight !== nothing\n",
    "    @test sum(abs2, g_ps.mha.q_proj.weight) > 0\n",
    "\n",
    "    @test haskey(g_ps, :ff) && haskey(g_ps.ff, :layer_1)\n",
    "    @test g_ps.ff.layer_1.weight !== nothing\n",
    "    @test sum(abs2, g_ps.ff.layer_1.weight) > 0\n",
    "\n",
    "    # Gradient w.r.t. input X (use a closure over X)\n",
    "    function loss_X(Xin)\n",
    "        Y, _ = HRMCommon.transformer_forward!(blk, ps, st, Xin)\n",
    "        return sum(abs2, Y .- T) / length(Y)\n",
    "    end\n",
    "    gX = first(Zygote.gradient(loss_X, X))\n",
    "    @test size(gX) == size(X)\n",
    "    @test norm(gX) > 0\n",
    "end\n",
    "\n",
    "@testset \"HRMCommon - variable sequence length\" begin\n",
    "    d, B = 8, 2\n",
    "    X1 = randn(Float32, d, 5, B)\n",
    "    X2 = randn(Float32, d, 11, B)\n",
    "\n",
    "    blk = HRMCommon.make_transformer_block(d; nheads=2, ff_mult=2,\n",
    "        attention_dropout_probability=0.0, pos_kind=:sinusoidal)\n",
    "    ps, st = setup_parameters_and_state(rng, blk)\n",
    "\n",
    "    Y1, st1 = HRMCommon.transformer_forward!(blk, ps, st, X1)\n",
    "    @test size(Y1) == size(X1)\n",
    "\n",
    "    Y2, st2 = HRMCommon.transformer_forward!(blk, ps, st1, X2)\n",
    "    @test size(Y2) == size(X2)\n",
    "end\n",
    "\n",
    "@testset \"MHA attention weights sum to 1\" begin\n",
    "    d, L, B = 8, 5, 2\n",
    "    blk = HRMCommon.make_transformer_block(d; nheads=2, ff_mult=2, pos_kind=:none)\n",
    "    rng = MersenneTwister(0)\n",
    "    ps, st = begin\n",
    "        ps_tmp = (;);\n",
    "        st_tmp = (;);\n",
    "        # small helper to setup all parts\n",
    "        function setup_all(rng, blk)\n",
    "            if blk.pos_layer === nothing\n",
    "                pos_ps, pos_st = (;), (;)\n",
    "            else\n",
    "                pos_ps, pos_st = Lux.setup(rng, blk.pos_layer)\n",
    "            end\n",
    "            mha_ps, mha_st = Lux.setup(rng, blk.mha)\n",
    "            ln1_ps, ln1_st = Lux.setup(rng, blk.ln1)\n",
    "            ff_ps,  ff_st  = Lux.setup(rng, blk.ff)\n",
    "            ln2_ps, ln2_st = Lux.setup(rng, blk.ln2)\n",
    "            ps = (; pos_layer=pos_ps, mha=mha_ps, ln1=ln1_ps, ff=ff_ps, ln2=ln2_ps)\n",
    "            st = (; pos_layer=pos_st, mha=mha_st, ln1=ln1_st, ff=ff_st, ln2=ln2_st)\n",
    "            return ps, st\n",
    "        end\n",
    "\n",
    "        setup_all(rng, blk)\n",
    "    end\n",
    "\n",
    "    X = randn(Float32, d, L, B)\n",
    "    Xn1, _ = HRMCommon.apply_tokenwise(blk.ln1, ps.ln1, st.ln1, X)\n",
    "    (A, attn), _ = Lux.apply(blk.mha, (Xn1, Xn1, Xn1), ps.mha, st.mha)\n",
    "\n",
    "    sz = size(attn)\n",
    "    candidates = [ax for ax in 1:length(sz) if sz[ax] == L]\n",
    "\n",
    "    # For each candidate axis, sum along it and measure how close sums are to 1.\n",
    "    errors = Float32[]\n",
    "    for ax in candidates\n",
    "        summed = sum(attn; dims=ax)\n",
    "        rows   = dropdims(summed; dims=ax)\n",
    "        push!(errors, maximum(abs, rows .- 1f0))\n",
    "    end\n",
    "\n",
    "    # Require that at least one axis produces ~1 row-sums\n",
    "    @test minimum(errors) <= 1f-4\n",
    "\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c7fec665",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e8d7c92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6acbd639",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b9cb2ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "030c1c41",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34099680",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c40b8c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f81bb3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "770c9aad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb601064",
   "metadata": {},
   "outputs": [],
   "source": [
    "using Plots, Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9985e219",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data generator\n",
    "# y = w^T x + 0.1 * ||x||^2 + eps, where ( eps is N(0, 0.01) )\n",
    "\n",
    "struct ToyGen\n",
    "    w::Vector{Float32}\n",
    "end\n",
    "\n",
    "function ToyGen(d_in::Int; seed::Int=123)\n",
    "    Random.seed!(seed)\n",
    "    w = randn(Float32, d_in)\n",
    "    return ToyGen(w)\n",
    "end\n",
    "\n",
    "#returns (x, y) with shapes (d_in, batch), (1, batch)\n",
    "function sample!(gen::ToyGen, batch::Int)\n",
    "    d = length(gen.w)\n",
    "    x = randn(Float32, d, batch)\n",
    "    # linear part\n",
    "    y_lin = gen.w' * x      # (1, batch)\n",
    "    # small quadratic interaction\n",
    "    y_quad = 0.1f0 .* sum(abs2, x; dims=1)    # (1, batch)\n",
    "    # noise\n",
    "    eps = 0.01f0 .* randn(Float32, 1, batch)\n",
    "    y = Float32.(y_lin .+ y_quad .+ eps)\n",
    "    return x, y\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11dd683f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    quantize_to_tokens(x; num_tokens::Int, lo::Real, hi::Real)\n",
    "\n",
    "Uniformly bins each element of x in [lo, hi] into 1..num_tokens.\n",
    "Clamps values outside [lo, hi] to the nearest edge bin.\n",
    "\"\"\"\n",
    "LO = -3.0\n",
    "HI =  3.0\n",
    "\n",
    "function quantize_to_tokens(x; num_tokens::Int, lo::Real, hi::Real)\n",
    "    if num_tokens ≤ 0\n",
    "        error(\"quantize_to_tokens called with num_tokens <= 0; use raw floats instead.\")\n",
    "    end\n",
    "    @assert num_tokens ≥ 2\n",
    "    xn = @. clamp((x - lo) / (hi - lo + eps(eltype(x))), 0, 1)\n",
    "    ids = floor.(Int, xn * (num_tokens - 1)) .+ 1\n",
    "    return ids\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40cd63c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "CFG = (\n",
    "    d_in   = 16,\n",
    "    d_hid  = 64,\n",
    "    d_out  = 1,\n",
    "    N      = 2,\n",
    "    T      = 3,\n",
    "    M      = 1,\n",
    "    batch  = 64,\n",
    "    lr     = 1e-3,\n",
    "    steps  = 300,\n",
    "    seed   = 42,\n",
    "\n",
    "    # input encoding\n",
    "    num_tokens = 0,     # set >0 to use embeddings with IDs; 0 = raw float encoder\n",
    "    d_embed    = 32,\n",
    "\n",
    "    # transformer hyperparameters (shared or separated)\n",
    "    l_heads    = 2,     # L-module heads\n",
    "    l_ff_mult  = 4,     # FFN expansion for L\n",
    "    h_heads    = 2,     # H-module heads\n",
    "    h_ff_mult  = 4,     # FFN expansion for H\n",
    "    dropout    = 0.0\n",
    ")\n",
    "\n",
    "\n",
    "Random.seed!(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "890af7ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Promote 2-D (d,B) to 3-D (d,1,B); remember if we promoted\n",
    "_as3d(X) = ndims(X) == 2 ? (reshape(X, size(X,1), 1, size(X,2)), true)  :\n",
    "          ndims(X) == 3 ? (X, false) :\n",
    "          error(\"Expected 2-D or 3-D tensor, got ndims=$(ndims(X))\")\n",
    "\n",
    "# Apply a (d,B) layer tokenwise over (d,L,B)\n",
    "function apply_tokenwise(layer, ps_layer, st_layer, X)\n",
    "    X3, was2d = _as3d(X)\n",
    "    d, L, B = size(X3)\n",
    "    X2 = reshape(X3, d, L*B)\n",
    "    Y2, st2 = Lux.apply(layer, X2, ps_layer, st_layer)\n",
    "    Y3 = reshape(Y2, d, L, B)\n",
    "    return was2d ? dropdims(Y3; dims=2) : Y3, st2\n",
    "end\n",
    "\n",
    "# Apply a (d,B) Chain tokenwise over (d,L,B)\n",
    "function apply_tokenwise_chain(chain, ps_chain, st_chain, X)\n",
    "    X3, was2d = _as3d(X)\n",
    "    d, L, B = size(X3)\n",
    "    X2 = reshape(X3, d, L*B)\n",
    "    Y2, st2 = Lux.apply(chain, X2, ps_chain, st_chain)\n",
    "    Y3 = reshape(Y2, d, L, B)\n",
    "    return was2d ? dropdims(Y3; dims=2) : Y3, st2\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11854f7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "068caffb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a04aca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b65bcf3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e80d749",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fead830",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d527255",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec6b4164",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b204d59f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a33bfdc3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a83ad31f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3989d69f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268f1764",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b53bec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7749655",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.10.9",
   "language": "julia",
   "name": "julia-1.10"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
