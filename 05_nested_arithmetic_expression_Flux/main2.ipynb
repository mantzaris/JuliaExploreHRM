{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "29d72de6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eaacb8a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m  Activating\u001b[22m\u001b[39m project at `~/Documents/repos/JuliaExploreHRM`\n"
     ]
    }
   ],
   "source": [
    "using Pkg\n",
    "Pkg.activate(\"..\")  #one level up, where Project.toml lives\n",
    "Pkg.instantiate()   #download/install anything missing\n",
    "# Pkg.status();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6261d600",
   "metadata": {},
   "outputs": [],
   "source": [
    "include(joinpath(@__DIR__, \"..\", \"data\", \"nested_boolean_gen.jl\"))\n",
    "include(joinpath(@__DIR__, \"..\", \"data\", \"hrm_common_nested_boolean_FLUX.jl\"))\n",
    "\n",
    "using .BooleanDataGenerator\n",
    "using .HRMFlux\n",
    "\n",
    "using Random, Statistics\n",
    "using Flux, Zygote, Optimisers\n",
    "using Flux: onehotbatch, onecold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a877509f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([1 1 … 1 1; 1 0 … 0 0; … ; 0 1 … 1 0; 0 1 … 0 1], [1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1], [\"(NOT (AND x1 (XOR x1 x1)))\", \"(AND x5 x2)\", \"(OR x1 x3)\", \"(OR x5 (AND x4 x1))\", \"(OR x5 (XOR x3 x4))\", \"(XOR x3 x2)\", \"(AND x5 x4)\", \"(OR (XOR x2 x3) (OR x5 x4))\", \"(AND x2 (NOT x5))\", \"(OR x3 (OR x4 x5))\", \"(AND x4 (AND (XOR x4 x3) x3))\", \"(AND (OR x3 x5) x3)\", \"(NOT (XOR x1 x5))\", \"(NOT (OR (XOR x5 x5) x1))\", \"(XOR x1 x1)\", \"(XOR x3 x1)\", \"(OR x4 (XOR x4 x1))\", \"(NOT x5)\", \"(XOR x3 x2)\", \"(OR x2 x1)\"])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Training data (depth 2-4)\n",
    "X_train, y_trainainainain, _ = generate_data(100; min_depth=2, max_depth=4)\n",
    "\n",
    "# Test data (depth 5-8) \n",
    "X_test, y_test, _ = generate_data(20; min_depth=5, max_depth=8)\n",
    "\n",
    "# Test data (held-out NAND)\n",
    "X_test_ops, y_test_ops, _ = generate_data(20; held_out_ops=[:NAND])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ce810bc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: X=(100, 5), y=(100,)\n",
      "Test (depth): X=(20, 5), y=(20,)\n",
      "Test (ops): X=(20, 5), y=(20,)\n",
      "\n",
      "Training examples:\n",
      "(NOT (AND x1 (NAND x1 x1))) | vars=[1, 1, 1, 1, 1] → 1\n",
      "(AND x5 x2) | vars=[1, 0, 0, 0, 0] → 0\n",
      "(OR x1 x3) | vars=[1, 1, 1, 0, 1] → 1\n"
     ]
    }
   ],
   "source": [
    "# Generate training data (depth 2-4)\n",
    "X_train, y_train, expr_train = generate_data(100; min_depth=2, max_depth=4, seed=42)\n",
    "\n",
    "# Generate test data with depth generalization (depth 5-8)\n",
    "X_test, y_test, expr_test = generate_data(20; min_depth=5, max_depth=8, seed=123)\n",
    "\n",
    "# Generate test data with held-out operations (no NAND)\n",
    "X_test_ops, y_test_ops, expr_test_ops = generate_data(20; held_out_ops=[:NAND], seed=456)\n",
    "\n",
    "println(\"Training: X=$(size(X_train)), y=$(size(y_train))\")\n",
    "println(\"Test (depth): X=$(size(X_test)), y=$(size(y_test))\")\n",
    "println(\"Test (ops): X=$(size(X_test_ops)), y=$(size(y_test_ops))\")\n",
    "\n",
    "# Show a few examples\n",
    "println(\"\\nTraining examples:\")\n",
    "for i in 1:3\n",
    "    println(\"$(expr_train[i]) | vars=$(X_train[i,:]) → $(y_train[i])\")\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee8a752a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0 1 … 0 1; 0 0 … 1 1; … ; 1 0 … 0 0; 0 1 … 0 0], [0, 0, 1, 1, 0, 1, 1, 0, 0, 1  …  1, 1, 1, 1, 0, 1, 0, 0, 0, 1], [\"(AND (XOR (OR x5 x5) (AND (AND x2 x5) (AND x5 x6))) (NOT (XOR x6 (NOT x4))))\", \"(NOT (OR x3 (OR x3 (NOT x4))))\", \"(OR (XOR x4 (OR (OR x6 x2) (AND x5 x1))) x2)\", \"(NOT (XOR x4 (NOT (AND x2 x6))))\", \"(NAND (XOR x6 (OR (NOT (OR x2 x2)) (NAND x3 x1))) (NAND x4 x6))\", \"(XOR (NOT (XOR (AND x6 x6) x3)) x1)\", \"(OR (OR x1 (AND x6 x2)) (AND (AND x3 x2) (XOR x6 (XOR x2 x4))))\", \"(NOT (XOR x1 (NAND (NAND x6 x6) x2)))\", \"(AND x4 (NOT (NAND (NAND x1 x6) x4)))\", \"(OR x6 (OR (OR x5 (NAND (OR x1 x2) (NOT x6))) (NOT (OR x4 x4))))\"  …  \"(XOR x2 (XOR (NAND x5 (XOR (AND x2 x3) x1)) (OR x5 x3)))\", \"(AND (NAND x1 (NAND (NOT x4) (NAND (NAND x6 x4) (NOT (NAND x4 x5))))) x4)\", \"(XOR (XOR (XOR x3 (XOR x1 x1)) x3) x2)\", \"(NOT (NOT (OR (NAND (NOT x4) x6) (AND x6 x2))))\", \"(AND (OR (OR (XOR x3 x1) (OR (XOR x6 x6) x2)) (AND (AND x6 x4) x2)) (NAND x1 x2))\", \"(NOT (NOT (NOT (XOR x5 x5))))\", \"(XOR (AND (XOR x5 x2) (NAND (XOR x2 x5) x4)) (NAND x4 (OR x3 x1)))\", \"(NOT (NOT (AND (NAND x2 x1) x6)))\", \"(OR x3 (NAND (XOR (AND x3 x5) x4) (NAND (XOR x2 x2) x1)))\", \"(NOT (XOR (OR (AND x1 x6) (XOR x2 x6)) x2))\"])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Train (depth 2-4), Test-ID (2-4), Test-OOD (5-8)\n",
    "X_train, y_train, expr_train = BooleanDataGenerator.generate_data(2000; variable_count=6, min_depth=2, max_depth=4, seed=1)\n",
    "X_id,    y_test_id,    expr_id    = BooleanDataGenerator.generate_data(500;  variable_count=6, min_depth=2, max_depth=4, seed=2)\n",
    "X_ood,   y_ood,   expr_ood   = BooleanDataGenerator.generate_data(500;  variable_count=6, min_depth=5, max_depth=8, seed=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c86bec99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500-element Vector{Vector{String}}:\n",
       " [\"(\", \"AND\", \"(\", \"XOR\", \"(\", \"OR\", \"x5=0\", \"x5=0\", \")\", \"(\"  …  \"(\", \"XOR\", \"x6=1\", \"(\", \"NOT\", \"x4=1\", \")\", \")\", \")\", \")\"]\n",
       " [\"(\", \"NOT\", \"(\", \"OR\", \"x3=0\", \"(\", \"OR\", \"x3=0\", \"(\", \"NOT\", \"x4=0\", \")\", \")\", \")\", \")\"]\n",
       " [\"(\", \"OR\", \"(\", \"XOR\", \"x4=0\", \"(\", \"OR\", \"(\", \"OR\", \"x6=0\"  …  \")\", \"(\", \"AND\", \"x5=0\", \"x1=0\", \")\", \")\", \")\", \"x2=1\", \")\"]\n",
       " [\"(\", \"NOT\", \"(\", \"XOR\", \"x4=1\", \"(\", \"NOT\", \"(\", \"AND\", \"x2=0\", \"x6=0\", \")\", \")\", \")\", \")\"]\n",
       " [\"(\", \"NAND\", \"(\", \"XOR\", \"x6=0\", \"(\", \"OR\", \"(\", \"NOT\", \"(\"  …  \"x1=0\", \")\", \")\", \")\", \"(\", \"NAND\", \"x4=0\", \"x6=0\", \")\", \")\"]\n",
       " [\"(\", \"XOR\", \"(\", \"NOT\", \"(\", \"XOR\", \"(\", \"AND\", \"x6=1\", \"x6=1\", \")\", \"x3=0\", \")\", \")\", \"x1=1\", \")\"]\n",
       " [\"(\", \"OR\", \"(\", \"OR\", \"x1=1\", \"(\", \"AND\", \"x6=1\", \"x2=1\", \")\"  …  \"XOR\", \"x6=1\", \"(\", \"XOR\", \"x2=1\", \"x4=1\", \")\", \")\", \")\", \")\"]\n",
       " [\"(\", \"NOT\", \"(\", \"XOR\", \"x1=0\", \"(\", \"NAND\", \"(\", \"NAND\", \"x6=1\", \"x6=1\", \")\", \"x2=1\", \")\", \")\", \")\"]\n",
       " [\"(\", \"AND\", \"x4=0\", \"(\", \"NOT\", \"(\", \"NAND\", \"(\", \"NAND\", \"x1=0\", \"x6=0\", \")\", \"x4=0\", \")\", \")\", \")\"]\n",
       " [\"(\", \"OR\", \"x6=1\", \"(\", \"OR\", \"(\", \"OR\", \"x5=0\", \"(\", \"NAND\"  …  \"(\", \"NOT\", \"(\", \"OR\", \"x4=0\", \"x4=0\", \")\", \")\", \")\", \")\"]\n",
       " ⋮\n",
       " [\"(\", \"AND\", \"(\", \"NAND\", \"x1=0\", \"(\", \"NAND\", \"(\", \"NOT\", \"x4=1\"  …  \"NAND\", \"x4=1\", \"x5=1\", \")\", \")\", \")\", \")\", \")\", \"x4=1\", \")\"]\n",
       " [\"(\", \"XOR\", \"(\", \"XOR\", \"(\", \"XOR\", \"x3=1\", \"(\", \"XOR\", \"x1=0\", \"x1=0\", \")\", \")\", \"x3=1\", \")\", \"x2=1\", \")\"]\n",
       " [\"(\", \"NOT\", \"(\", \"NOT\", \"(\", \"OR\", \"(\", \"NAND\", \"(\", \"NOT\"  …  \"x6=1\", \")\", \"(\", \"AND\", \"x6=1\", \"x2=1\", \")\", \")\", \")\", \")\"]\n",
       " [\"(\", \"AND\", \"(\", \"OR\", \"(\", \"OR\", \"(\", \"XOR\", \"x3=1\", \"x1=1\"  …  \")\", \"x2=0\", \")\", \")\", \"(\", \"NAND\", \"x1=1\", \"x2=0\", \")\", \")\"]\n",
       " [\"(\", \"NOT\", \"(\", \"NOT\", \"(\", \"NOT\", \"(\", \"XOR\", \"x5=0\", \"x5=0\", \")\", \")\", \")\", \")\"]\n",
       " [\"(\", \"XOR\", \"(\", \"AND\", \"(\", \"XOR\", \"x5=1\", \"x2=1\", \")\", \"(\"  …  \"(\", \"NAND\", \"x4=1\", \"(\", \"OR\", \"x3=0\", \"x1=1\", \")\", \")\", \")\"]\n",
       " [\"(\", \"NOT\", \"(\", \"NOT\", \"(\", \"AND\", \"(\", \"NAND\", \"x2=1\", \"x1=0\", \")\", \"x6=0\", \")\", \")\", \")\"]\n",
       " [\"(\", \"OR\", \"x3=0\", \"(\", \"NAND\", \"(\", \"XOR\", \"(\", \"AND\", \"x3=0\"  …  \"NAND\", \"(\", \"XOR\", \"x2=0\", \"x2=0\", \")\", \"x1=1\", \")\", \")\", \")\"]\n",
       " [\"(\", \"NOT\", \"(\", \"XOR\", \"(\", \"OR\", \"(\", \"AND\", \"x1=0\", \"x6=0\", \")\", \"(\", \"XOR\", \"x2=1\", \"x6=0\", \")\", \")\", \"x2=1\", \")\", \")\"]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function tokenize_with_assignment(expression::String, variable_row::Vector{Int})\n",
    "    spaced = replace(expression, r\"([()])\" => s\" \\1 \")\n",
    "    raw_tokens = split(strip(spaced))\n",
    "    tokens = String[]\n",
    "    for t in raw_tokens\n",
    "        if startswith(t, \"x\")\n",
    "            idx = parse(Int, t[2:end])\n",
    "            push!(tokens, \"x$(idx)=$(variable_row[idx])\")\n",
    "        else\n",
    "            push!(tokens, t)\n",
    "        end\n",
    "    end\n",
    "    return tokens\n",
    "end\n",
    "\n",
    "# Build token sequences\n",
    "function build_token_sequences(expressions::Vector{String}, X::Array{Int,2})\n",
    "    seqs = Vector{Vector{String}}(undef, length(expressions))\n",
    "    for i in eachindex(expressions)\n",
    "        seqs[i] = tokenize_with_assignment(expressions[i], vec(X[i, :]))\n",
    "    end\n",
    "    return seqs\n",
    "end\n",
    "\n",
    "train_tokens = build_token_sequences(expr_train, X_train)\n",
    "id_tokens    = build_token_sequences(expr_id,    X_id)\n",
    "ood_tokens   = build_token_sequences(expr_ood,   X_ood)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b6bebf20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500-element Vector{Vector{Int64}}:\n",
       " [1, 2, 1, 14, 1, 8, 4, 4, 5, 1  …  1, 14, 3, 1, 16, 7, 5, 5, 5, 5]\n",
       " [1, 16, 1, 8, 15, 1, 8, 15, 1, 16, 13, 5, 5, 5, 5]\n",
       " [1, 8, 1, 14, 13, 1, 8, 1, 8, 9  …  5, 1, 2, 4, 18, 5, 5, 5, 12, 5]\n",
       " [1, 16, 1, 14, 7, 1, 16, 1, 2, 6, 9, 5, 5, 5, 5]\n",
       " [1, 11, 1, 14, 9, 1, 8, 1, 16, 1  …  18, 5, 5, 5, 1, 11, 13, 9, 5, 5]\n",
       " [1, 14, 1, 16, 1, 14, 1, 2, 3, 3, 5, 15, 5, 5, 19, 5]\n",
       " [1, 8, 1, 8, 19, 1, 2, 3, 12, 5  …  14, 3, 1, 14, 12, 7, 5, 5, 5, 5]\n",
       " [1, 16, 1, 14, 18, 1, 11, 1, 11, 3, 3, 5, 12, 5, 5, 5]\n",
       " [1, 2, 13, 1, 16, 1, 11, 1, 11, 18, 9, 5, 13, 5, 5, 5]\n",
       " [1, 8, 3, 1, 8, 1, 8, 4, 1, 11  …  1, 16, 1, 8, 13, 13, 5, 5, 5, 5]\n",
       " ⋮\n",
       " [1, 2, 1, 11, 18, 1, 11, 1, 16, 7  …  11, 7, 17, 5, 5, 5, 5, 5, 7, 5]\n",
       " [1, 14, 1, 14, 1, 14, 10, 1, 14, 18, 18, 5, 5, 10, 5, 12, 5]\n",
       " [1, 16, 1, 16, 1, 8, 1, 11, 1, 16  …  3, 5, 1, 2, 3, 12, 5, 5, 5, 5]\n",
       " [1, 2, 1, 8, 1, 8, 1, 14, 10, 19  …  5, 6, 5, 5, 1, 11, 19, 6, 5, 5]\n",
       " [1, 16, 1, 16, 1, 16, 1, 14, 4, 4, 5, 5, 5, 5]\n",
       " [1, 14, 1, 2, 1, 14, 17, 12, 5, 1  …  1, 11, 7, 1, 8, 15, 19, 5, 5, 5]\n",
       " [1, 16, 1, 16, 1, 2, 1, 11, 12, 18, 5, 9, 5, 5, 5]\n",
       " [1, 8, 15, 1, 11, 1, 14, 1, 2, 15  …  11, 1, 14, 6, 6, 5, 19, 5, 5, 5]\n",
       " [1, 16, 1, 14, 1, 8, 1, 2, 18, 9, 5, 1, 14, 12, 9, 5, 5, 12, 5, 5]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Vocabulary from training tokens only\n",
    "function build_vocab(token_sequences::Vector{Vector{String}})\n",
    "    vocab = Dict{String,Int}()\n",
    "    next_id = 1\n",
    "    for seq in token_sequences\n",
    "        for t in seq\n",
    "            if !haskey(vocab, t)\n",
    "                vocab[t] = next_id\n",
    "                next_id += 1\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    return vocab\n",
    "end\n",
    "\n",
    "vocab = build_vocab(train_tokens)\n",
    "unk_id = length(vocab) + 1  # just in case (should not be needed here)\n",
    "\n",
    "# Map tokens to integer ids\n",
    "token_to_id_tmp = t -> get(vocab, t, unk_id)\n",
    "function map_to_ids(token_sequences::Vector{Vector{String}})\n",
    "    return [map(token_to_id_tmp, seq) for seq in token_sequences]\n",
    "end\n",
    "\n",
    "train_ids = map_to_ids(train_tokens)\n",
    "id_ids    = map_to_ids(id_tokens)\n",
    "ood_ids   = map_to_ids(ood_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f45d188b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dd803fca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfbc0c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "VARIABLE_COUNT = 6\n",
    "N_A   = 2000   # Train Phase A (2-4)\n",
    "N_B   = 3000   # Train Phase B (2-6)\n",
    "N_ID  = 1000   # Test (2-4)\n",
    "N_MID = 1000   # Test (5-6)\n",
    "N_OOD = 1000   # Test (7-8)\n",
    "\n",
    "Random.seed!(1)\n",
    "\n",
    "\n",
    "# Train pools\n",
    "X_tr_A,  y_tr_A,  expr_tr_A  = BooleanDataGenerator.generate_data(N_A;  variable_count=VARIABLE_COUNT, min_depth=2, max_depth=4, seed=101)\n",
    "X_tr_B,  y_tr_B,  expr_tr_B  = BooleanDataGenerator.generate_data(N_B;  variable_count=VARIABLE_COUNT, min_depth=2, max_depth=6, seed=102)\n",
    "\n",
    "# Fixed test sets\n",
    "X_te_ID,  y_te_ID,  expr_te_ID  = BooleanDataGenerator.generate_data(N_ID;  variable_count=VARIABLE_COUNT, min_depth=2, max_depth=4, seed=201)\n",
    "X_te_MID, y_te_MID, expr_te_MID = BooleanDataGenerator.generate_data(N_MID; variable_count=VARIABLE_COUNT, min_depth=5, max_depth=6, seed=202)\n",
    "X_te_OOD, y_te_OOD, expr_te_OOD = BooleanDataGenerator.generate_data(N_OOD; variable_count=VARIABLE_COUNT, min_depth=7, max_depth=8, seed=203)\n",
    "\n",
    "\n",
    "# 2) Tokenize with assignment, build vocab, pad to fixed length\n",
    "function tokenize_with_assignment(expression::String, variable_row::AbstractVector{<:Integer})\n",
    "    spaced = replace(expression, r\"([()])\" => s\" \\1 \")\n",
    "    raw_tokens = split(strip(spaced))\n",
    "    tokens = String[]\n",
    "    for t in raw_tokens\n",
    "        if startswith(t, \"x\")\n",
    "            idx = parse(Int, t[2:end])\n",
    "            push!(tokens, \"x$(idx)=$(variable_row[idx])\")\n",
    "        else\n",
    "            push!(tokens, t)  # \"(\", \")\", \"AND\", \"OR\", \"XOR\", \"NAND\", \"NOT\"\n",
    "        end\n",
    "    end\n",
    "    return tokens\n",
    "end\n",
    "\n",
    "function build_token_sequences(expressions::Vector{String}, X::Array{Int,2})\n",
    "    seqs = Vector{Vector{String}}(undef, length(expressions))\n",
    "    for i in eachindex(expressions)\n",
    "        seqs[i] = tokenize_with_assignment(expressions[i], vec(X[i, :]))\n",
    "    end\n",
    "    return seqs\n",
    "end\n",
    "\n",
    "\n",
    "# Tokenize every split\n",
    "tokens_tr_A  = build_token_sequences(expr_tr_A,  X_tr_A)\n",
    "tokens_tr_B  = build_token_sequences(expr_tr_B,  X_tr_B)\n",
    "tokens_te_ID = build_token_sequences(expr_te_ID, X_te_ID)\n",
    "tokens_te_M  = build_token_sequences(expr_te_MID,X_te_MID)\n",
    "tokens_te_O  = build_token_sequences(expr_te_OOD,X_te_OOD)\n",
    "\n",
    "\n",
    "# Vocabulary from training only, with <PAD>=1 and <UNK>=2\n",
    "function build_vocab_with_specials(train_token_sequences::Vector{Vector{String}})\n",
    "    vocab = Dict{String,Int}(\"<PAD>\"=>1, \"<UNK>\"=>2)\n",
    "    next_id = 3\n",
    "    for seq in train_token_sequences\n",
    "        for t in seq\n",
    "            haskey(vocab, t) || (vocab[t] = next_id; next_id += 1)\n",
    "        end\n",
    "    end\n",
    "    return vocab\n",
    "end\n",
    "\n",
    "\n",
    "# Vocab ONLY from Phase-A training (guards against leakage)\n",
    "vocab = build_vocab_with_specials(tokens_tr_A)\n",
    "const PAD_ID = vocab[\"<PAD>\"]\n",
    "const UNK_ID = vocab[\"<UNK>\"]\n",
    "token_to_id(t) = get(vocab, t, UNK_ID)\n",
    "\n",
    "\n",
    "# Per-split padding (no global max)\n",
    "function to_ids(tokens::Vector{Vector{String}})\n",
    "    Lmax = maximum(length.(tokens))\n",
    "    n = length(tokens)\n",
    "    Xids = fill(PAD_ID, Lmax, n)\n",
    "    @inbounds for i in 1:n\n",
    "        s = tokens[i]\n",
    "        for j in 1:length(s)\n",
    "            Xids[j, i] = token_to_id(s[j])\n",
    "        end\n",
    "    end\n",
    "    return Xids\n",
    "end\n",
    "\n",
    "Xids_tr_A  = to_ids(tokens_tr_A)\n",
    "Xids_tr_B  = to_ids(tokens_tr_B)\n",
    "Xids_te_ID = to_ids(tokens_te_ID)\n",
    "Xids_te_M  = to_ids(tokens_te_M)\n",
    "Xids_te_O  = to_ids(tokens_te_O)\n",
    "\n",
    "\n",
    "token_to_id(t) = get(vocab, t, UNK_ID)\n",
    "\n",
    "# function to_padded_id_matrix(token_sequences::Vector{Vector{String}}, max_len::Int)\n",
    "#     n = length(token_sequences)\n",
    "#     Xids = fill(PAD_ID, max_len, n)  # (sequence_len, n_samples)\n",
    "#     for i in 1:n\n",
    "#         seq = token_sequences[i]\n",
    "#         L = min(length(seq), max_len)\n",
    "#         @inbounds for j in 1:L\n",
    "#             Xids[j, i] = token_to_id(seq[j])\n",
    "#         end\n",
    "#     end\n",
    "#     return Xids\n",
    "# end\n",
    "\n",
    "# token_ids_train = to_padded_id_matrix(tokens_train, global_max_len)\n",
    "# token_ids_id    = to_padded_id_matrix(tokens_id,    global_max_len)\n",
    "# token_ids_ood   = to_padded_id_matrix(tokens_ood,   global_max_len)\n",
    "\n",
    "\n",
    "cfg = (\n",
    "    d_in   = 0,\n",
    "    d_hid  = 96,\n",
    "    d_out  = 1,\n",
    "    N      = 3,\n",
    "    T      = 0,\n",
    "    batch  = 32,\n",
    "    lr     = 1e-3,\n",
    "    num_tokens = length(vocab),\n",
    "    d_embed    = 64,\n",
    "    l_heads    = 2,\n",
    "    l_ff_mult  = 4,\n",
    "    h_heads    = 2,\n",
    "    h_ff_mult  = 4,\n",
    "    dropout    = 0.0,\n",
    "    pad_id     = PAD_ID\n",
    ")\n",
    "\n",
    "# Compute the maximum sequence length across *all* splits\n",
    "L_A   = size(Xids_tr_A,  1)\n",
    "L_B   = size(Xids_tr_B,  1)\n",
    "L_ID  = size(Xids_te_ID, 1)\n",
    "L_MID = size(Xids_te_M,  1)\n",
    "L_OOD = size(Xids_te_O,  1)\n",
    "\n",
    "# +1 if you use CLS (the CLS token is prepended to H_in)\n",
    "POS_L_MAX = max(L_A, L_B, L_ID, L_MID, L_OOD) + 1\n",
    "\n",
    "models = HRMFlux.build_models(\n",
    "    cfg; positional_encoding_kind = :sinusoidal, pos_L_max = POS_L_MAX\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "function seq_lengths_from_pad_id(Xids::AbstractMatrix{<:Integer}, pad_id::Int)\n",
    "    L, B = size(Xids)\n",
    "    lengths = fill(L, B)\n",
    "    @inbounds for b in 1:B\n",
    "        for t in 1:L\n",
    "            if Xids[t, b] == pad_id\n",
    "                lengths[b] = t - 1\n",
    "                break\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    return lengths\n",
    "end\n",
    "\n",
    "\n",
    "\n",
    "# keep PAD embedding at zero to avoid length bias\n",
    "if models.tok_emb !== nothing\n",
    "    models.tok_emb.weight[:, PAD_ID] .= 0f0\n",
    "end\n",
    "\n",
    "\n",
    "function each_minibatch(X::AbstractMatrix{<:Integer}, y::AbstractVector{<:Integer}, batch_size::Int)\n",
    "    idx = collect(1:size(X, 2))\n",
    "    Random.shuffle!(idx)\n",
    "    out = Vector{Tuple{Matrix{Int}, Vector{Int}}}()\n",
    "    for k in 1:batch_size:length(idx)\n",
    "        sel = idx[k:min(k+batch_size-1, length(idx))]\n",
    "        push!(out, (X[:, sel], y[sel]))\n",
    "    end\n",
    "    return out\n",
    "end\n",
    "\n",
    "\n",
    "\n",
    "function batch_loss(models, x_batch::AbstractMatrix{<:Integer}, y_batch::AbstractVector{<:Integer}, cfg)\n",
    "    batch_size = size(x_batch, 2)\n",
    "    low_state, high_state = HRMFlux.init_states(batch_size, cfg.d_hid)\n",
    "    \n",
    "    # yhat, _, _ = HRMFlux.run_segment_GRU!(models, x_batch, low_state, high_state; N=cfg.N, T=cfg.T, cfg=cfg)\n",
    "    yhat, _, _ = HRMFlux.run_sequence_segment!(models, x_batch, low_state, high_state; N=cfg.N, cfg=cfg)\n",
    "\n",
    "    targets = reshape(Float32.(y_batch), 1, batch_size)\n",
    "    return Flux.logitbinarycrossentropy(yhat, targets)\n",
    "end\n",
    "\n",
    "function accuracy(models, X::AbstractMatrix{<:Integer}, y::AbstractVector{<:Integer}, cfg; batch_size::Int=256)\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for (xb, yb) in each_minibatch(X, y, batch_size)\n",
    "        bs = size(xb, 2)\n",
    "        low_state, high_state = HRMFlux.init_states(bs, cfg.d_hid)\n",
    "\n",
    "        # yhat, _, _ = HRMFlux.run_segment_GRU!(models, xb, low_state, high_state; N=cfg.N, T=cfg.T, cfg=cfg)\n",
    "        yhat, _, _ = HRMFlux.run_sequence_segment!(models, xb, low_state, high_state; N=cfg.N, cfg=cfg)\n",
    "        \n",
    "        preds = @. yhat > 0  # threshold logit at 0\n",
    "        correct += sum(Int.(preds[1, :]) .== yb)\n",
    "        total   += bs\n",
    "    end\n",
    "    return correct / total\n",
    "end\n",
    "\n",
    "# 5) Train and evaluate\n",
    "# curriculum phases\n",
    "epochs_A = 15    # Phase A (2-4)\n",
    "epochs_B = 20    # Phase B (2-6)\n",
    "total_epochs = epochs_A + epochs_B\n",
    "\n",
    "opt_state = Optimisers.setup(Optimisers.Adam(cfg.lr), models)\n",
    "\n",
    "for epoch in 1:total_epochs\n",
    "    total_loss = 0.0; batches = 0\n",
    "\n",
    "    # pick the current train pool\n",
    "    Xtr, ytr = epoch <= epochs_A ? (Xids_tr_A, y_tr_A) : (Xids_tr_B, y_tr_B)\n",
    "\n",
    "    for (xb, yb) in each_minibatch(Xtr, ytr, cfg.batch)\n",
    "        # keep PAD column neutral (before & after update)\n",
    "        if hasproperty(models, :tok_emb) && models.tok_emb !== nothing\n",
    "            models.tok_emb.weight[:, PAD_ID] .= 0f0\n",
    "        end\n",
    "        L, back = Zygote.pullback(m -> batch_loss(m, xb, yb, cfg), models)\n",
    "        grads = back(one(L))[1]\n",
    "        opt_state, models = Optimisers.update(opt_state, models, grads)\n",
    "        if hasproperty(models, :tok_emb) && models.tok_emb !== nothing\n",
    "            models.tok_emb.weight[:, PAD_ID] .= 0f0\n",
    "        end\n",
    "        total_loss += Float64(L); batches += 1\n",
    "    end\n",
    "\n",
    "    acc_id  = accuracy(models, Xids_te_ID,  y_te_ID,  cfg)   # 2-4\n",
    "    acc_mid = accuracy(models, Xids_te_M,   y_te_MID, cfg)   # 5-6\n",
    "    acc_ood = accuracy(models, Xids_te_O,   y_te_OOD, cfg)   # 7-8\n",
    "\n",
    "    @info \"epoch=$(epoch)  loss=$(round(total_loss/batches, digits=4))  \" *\n",
    "          \"ID(2-4)=$(round(acc_id,digits=3))  MID(5-6)=$(round(acc_mid,digits=3))  OOD(7-8)=$(round(acc_ood,digits=3))\"\n",
    "end\n",
    "\n",
    "println(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a9388947",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L per split: (trA = 28, trB = 48, teID = 23, teMID = 50, teOOD = 81)\n",
      "(\"PAD trA\", 0.724625)(\" PAD trB\", 0.7914652777777778)(\" PAD teID\", 0.669)(\" PAD teMID\", 0.56496)(\" PAD teOOD\", 0.5720123456790124)\n"
     ]
    }
   ],
   "source": [
    "len = (trA=size(Xids_tr_A,1), trB=size(Xids_tr_B,1),\n",
    "       teID=size(Xids_te_ID,1), teMID=size(Xids_te_M,1), teOOD=size(Xids_te_O,1))\n",
    "println(\"L per split: \", len)\n",
    "\n",
    "padfrac(M, pad=PAD_ID) = count(==(pad), vec(M)) / length(M)\n",
    "println((\"PAD trA\", padfrac(Xids_tr_A)),\n",
    "        (\" PAD trB\", padfrac(Xids_tr_B)),\n",
    "        (\" PAD teID\", padfrac(Xids_te_ID)),\n",
    "        (\" PAD teMID\", padfrac(Xids_te_M)),\n",
    "        (\" PAD teOOD\", padfrac(Xids_te_O)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "556169d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce118198",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f95c6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19abec2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b65a31fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0008ac42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b29415f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c19ef3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e33b870c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfaad3d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b0c4417",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "017cdd8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b574560d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c8da7c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f426ff9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0862d47e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c659183",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330d44c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203a8e25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f813c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc38a2e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dcd4efd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1420cbb2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "954f40f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.10.9",
   "language": "julia",
   "name": "julia-1.10"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
