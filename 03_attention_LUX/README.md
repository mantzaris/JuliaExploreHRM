
Now here the example builds on the previous using an attention layer as well, and there's no mean pooling after the inputs